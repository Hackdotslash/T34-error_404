{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "breastcancer.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "26Dn6xpuHUJD"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmxfCfvXNqkb"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsXCSPFT4do_"
      },
      "source": [
        "!pip install wget\n",
        "!wget \"https://scholar.cu.edu.eg/Dataset_BUSI.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMf_-XgF6Kzr"
      },
      "source": [
        "!unzip -qq \"/content/Dataset_BUSI.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5TyIZyZKGbz"
      },
      "source": [
        "# ==== Install Dependencies\n",
        "!pip install -q efficientnet-pytorch\n",
        "!pip install -q albumentations \n",
        "!pip install -q pytorch_ranger"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCkFVHBTKXJ0"
      },
      "source": [
        "# ==== Import Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import seaborn as sns\n",
        "import random\n",
        "import os\n",
        "\n",
        "\n",
        "import albumentations as aug\n",
        "from albumentations.pytorch.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") \n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "\n",
        "from pytorch_ranger import Ranger\n",
        "\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "from pathlib import Path\n",
        "\n",
        "from torchvision import transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdNEttTexy7_"
      },
      "source": [
        "#Data Loader and Model optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OpdNPwrxxtC"
      },
      "source": [
        "dataset    = Cloader(image_path,targets,resize=None,transforms=None)\r\n",
        "\"\"\" returns {'image':tensor_image,'targets':tensor_labels} \"\"\"\r\n",
        "dataloader = DataLoader(dataset,batch_size=64\r\n",
        "                                     ,shuffle=True,num_workers=4)\r\n",
        "                                     \r\n",
        "\"\"\" using next(iter(dataloader)) returns a dictionary with keys 'image'\r\n",
        " and 'targets'.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HThsTA1x6Mn"
      },
      "source": [
        "es = EarlyStop(patience=7, mode=\"max\", delta=0.0001) \r\n",
        "\r\n",
        "\"\"\" Sample code \"\"\"\r\n",
        "for epoch in range(epochs):\r\n",
        "    epoch_score = Trainer.evaluate(......)\r\n",
        "    es(epoch_score , model , model_path =\"./best.pth\")\r\n",
        "    \"\"\" model_path is the location+filename to save the best model \"\"\"\r\n",
        "    if es.early_stop=True:\r\n",
        "\t    break\r\n",
        "\r\n",
        "es.reset()  \"\"\" resets the best_epoch_score, if in case training multiple\r\n",
        "                folds without creating 'es' object again and again.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyMal3LVyUcP"
      },
      "source": [
        "trainer        = Trainer(model,optimizer,device,train_scheduler = None,\r\n",
        "                 val_scheduler = None,accumulation_steps=1,fp16=False,\r\n",
        "                use_mean_loss=False,checkpoint=None,save_path = \"./\")\r\n",
        "                \r\n",
        "\"\"\" Training and Evaluating \"\"\"\r\n",
        "train_loss              = trainer.train(train_dataloader)\r\n",
        "y_true ,y_pred,val_loss = trainer.evaluate(val_dataloader)\r\n",
        "\r\n",
        "\"\"\" Prediction \"\"\"\r\n",
        "y_preds                 = trainer.predict(test_dataloader)  \r\n",
        "\r\n",
        "\"\"\" In depths \"\"\"\r\n",
        "\"\"\" train_scheduler/val_scheduler : call scheduler.step() while training\r\n",
        "                                    or after validating\r\n",
        "    accumulation_step             : implements gradient accumulation \r\n",
        "                                    (default = 1)\r\n",
        "    fp16                          : mixed precision training\r\n",
        "    use_mean_loss                 : loss.mean().backward()\r\n",
        "                                    (false if loss is already meaned)\r\n",
        "    checkpoint                    : torch.load(checkpoint),loads the \r\n",
        "                                    checkpoint and resumes training.\r\n",
        "    save_path                     : location to save the last epoch \r\n",
        "                                    weights                             \"\"\"\r\n",
        "\r\n",
        "\"\"\" Having problem with non resumable training epochs? \"\"\"    \r\n",
        "trainer.saver()  \"\"\" saves the last epoch to the save_path location \"\"\"\r\n",
        "\r\n",
        "\"\"\" dataloader must be set in the same state to resume training \r\n",
        "Use [https://gist.github.com/usamec/1b3b4dcbafad2d58faa71a9633eea6a5]\r\n",
        "an implementation of ResumableRandomSampler()  \"\"\"\r\n",
        "\r\n",
        "sampler = ResumableRandomSampler(dataset)\r\n",
        "loader  = DataLoader(dataset, batch_size=64, sampler=sampler)\r\n",
        "torch.save(sampler.get_state(), \"test_samp.pth\") -- save the state\r\n",
        "sampler.set_state(torch.load(\"test_samp.pth\"))  -- load and set the state\r\n",
        "\r\n",
        "\"\"\" and tadaa , resume training 100 % :) \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rE2_D3jkyYaG"
      },
      "source": [
        "logger   = Logger(path=\"./\")  \"\"\" save path for logger\"\"\"\r\n",
        "logger.write( message ,verbose = 1) \"\"\" verbose to print the message\"\"\"\r\n",
        "\r\n",
        "\"\"\" Helps Keep Track of Training \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC8m4ERhya8Z"
      },
      "source": [
        "saver    = Saver(path=\"./\" , mode = \"max\")\r\n",
        "\"\"\" saves the model, optimizer and scheduler based on score and mode \"\"\"\r\n",
        "\r\n",
        "saver.save(model,optimizer,scheduler,metric)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RGV9XKvyeFc"
      },
      "source": [
        "class Net(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Net, self).__init__()\r\n",
        "        self.base_model = timm.create_model('resnet18',pretrained=True,num_classes=1)\r\n",
        "    def forward(self, image, targets):\r\n",
        "        batch_size, _, _, _ = image.shape\r\n",
        "        out = self.base_model(image)\r\n",
        "        loss = nn.BCEWithLogitsLoss()(out.view(batch_size,), targets.type_as(out))\r\n",
        "        return out, loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T-IrGQh62jl"
      },
      "source": [
        "import os\n",
        "\n",
        "asps = []\n",
        "for root, dirs, files in os.walk('/content/Dataset_BUSI_with_GT/benign/'):\n",
        "    # print(root)\n",
        "    # print(dirs)\n",
        "    # print(files)\n",
        "    for file in files:\n",
        "        if file.endswith('.png'):\n",
        "            asps.append(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK505pha9icL"
      },
      "source": [
        "image_id=[]\n",
        "labels=[]\n",
        "for x in range(len(asps)):\n",
        "  asps[x]=os.path.join('/content/Dataset_BUSI_with_GT/benign',asps[x])\n",
        "  image_id.append(asps[x])\n",
        "  labels.append(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G3g8FPl-95z"
      },
      "source": [
        "import os\n",
        "\n",
        "asps = []\n",
        "for root, dirs, files in os.walk('/content/Dataset_BUSI_with_GT/malignant/'):\n",
        "    # print(root)\n",
        "    # print(dirs)\n",
        "    # print(files)\n",
        "    for file in files:\n",
        "        if file.endswith('.png'):\n",
        "            asps.append(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbSGTD5e_Vti"
      },
      "source": [
        "# image_id=[]\n",
        "for x in range(len(asps)):\n",
        "  asps[x]=os.path.join('/content/Dataset_BUSI_with_GT/malignant/',asps[x])\n",
        "  image_id.append(asps[x])\n",
        "  labels.append(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZr_8jDYAAEE"
      },
      "source": [
        "import os\n",
        "asps = []\n",
        "for root, dirs, files in os.walk('/content/Dataset_BUSI_with_GT/normal/'):\n",
        "    # print(root)\n",
        "    # print(dirs)\n",
        "    # print(files)\n",
        "    for file in files:\n",
        "        if file.endswith('.png'):\n",
        "            asps.append(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f4vZVANAWdS"
      },
      "source": [
        "# image_id=[]\n",
        "for x in range(len(asps)):\n",
        "  asps[x]=os.path.join('/content/Dataset_BUSI_with_GT/normal/',asps[x])\n",
        "  image_id.append(asps[x])\n",
        "  labels.append(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF0ciXLGAxrw"
      },
      "source": [
        "d={'image_id':image_id,'labels':labels}\n",
        "df=pd.DataFrame(d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo61ovOWB-NF"
      },
      "source": [
        "df=df.sample(frac=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTIvuHfDCSj5"
      },
      "source": [
        "df=df.reset_index(drop=True)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyuWoK-eNwYS"
      },
      "source": [
        "map={'Benign':0,'Malignant':1,'Normal':2}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iny4DZoKnYxP"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QhQHolYShOM"
      },
      "source": [
        "from sklearn.model_selection import train_test_split as tts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sX3Cb4RZDIyo"
      },
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "seed_everything(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2tX1Of8HUIf"
      },
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUsLanVCtenT"
      },
      "source": [
        "!mkdir \"/content/drive/MyDrive/HackNagpur/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xpje_XSoNzse"
      },
      "source": [
        "save_root = \"/content/drive/MyDrive/HackNagpur/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOIEEnXKOXDe"
      },
      "source": [
        "# training_data_path = \"/content/ISIC_2019_Training_Input/ISIC_2019_Training_Input\"\n",
        "\n",
        "X_train , X_val ,Y_train , Y_val = tts(df, df.labels.values, test_size=0.25\n",
        "                                       ,random_state=42,stratify=df.labels.values)\n",
        "X_train       = X_train.reset_index(drop=True)\n",
        "X_val         = X_val.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2Ua3O5fOW-I"
      },
      "source": [
        "# ===== Augmentations\n",
        "\n",
        "mean       = (0.485, 0.456, 0.406)\n",
        "std        = (0.229, 0.224, 0.225)\n",
        "image_size=224\n",
        "train_tfms = aug.Compose([\n",
        "            aug.Resize(224,224),\n",
        "            aug.RandomSizedCrop(min_max_height=(64,224),height=224,width=224,p=0.5),\n",
        "            aug.HorizontalFlip(p=0.5),\n",
        "            aug.RandomBrightnessContrast(0.1,0.1),\n",
        "            #aug.HueSaturationValue(10,10,10),\n",
        "            aug.RGBShift(),\n",
        "            aug.RandomContrast(limit=0.2),\n",
        "            aug.RandomGamma(),\n",
        "            #aug.ShiftScaleRotate(rotate_limit=(-45,45)),\n",
        "            aug.GaussNoise(p=0.35),\n",
        "            aug.IAASharpen(),\n",
        "            aug.ToGray(p=0.35),\n",
        "            aug.CLAHE(clip_limit=4.0, p=0.7),\n",
        "            #aug.RandomSizedCrop(min_max_height=(64,256),height=256,width=256,p=0.5),\n",
        "            aug.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5),\n",
        "            aug.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.85),\n",
        "            #aug.Cutout(max_h_size=int(image_size * 0.375), max_w_size=int(image_size * 0.375), num_holes=1, p=0.7),\n",
        "            aug.Normalize(mean,std,max_pixel_value=255.0,always_apply=True),\n",
        "            ])\n",
        "\n",
        "test_tfms  = aug.Compose([\n",
        "            aug.Resize(224,224),\n",
        "            aug.Normalize(mean,std,max_pixel_value=255.0,always_apply=True),\n",
        "            ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tOUhwfaO5fR"
      },
      "source": [
        "# training_data_path='/content/train'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkAucLGgI9aa"
      },
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "from PIL import ImageFile   #ImageFile contains support for PIL to open and save images\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES=True  #If image is truncated(or corrupt) then also load it..\n",
        "import torch\n",
        "\n",
        "class Cloader:\n",
        "    def __init__(self,image_path,targets,resize=None,transforms=None):\n",
        "        self.image_path=image_path\n",
        "        self.targets=targets\n",
        "        self.resize=resize\n",
        "        self.transforms=transforms\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_path)\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        image = Image.open(self.image_path[idx]).convert(\"RGB\")\n",
        "        targets = self.targets[idx]\n",
        "        if self.resize is not None:\n",
        "            image = image.resize(\n",
        "                (self.resize[1], self.resize[0]), resample=Image.BILINEAR\n",
        "            )\n",
        "        image = np.array(image)\n",
        "        if self.transforms is not None:\n",
        "            augmented = self.transforms(image=image)\n",
        "            image = augmented[\"image\"]\n",
        "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
        "        return {\n",
        "            \"image\": torch.tensor(image, dtype=torch.float),\n",
        "            \"targets\": torch.tensor(targets, dtype=torch.long),\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvu4IaecOW6a"
      },
      "source": [
        "train_images     = X_train.image_id.values.tolist()\n",
        "# train_images     = [os.path.join(training_data_path, i+\".png\") for i in train_images]\n",
        "\n",
        "test_images      = X_val.image_id.values.tolist()\n",
        "# test_images      = [os.path.join(training_data_path, i+\".png\") for i in test_images]\n",
        "\n",
        "train_dataset    = Cloader(train_images,X_train.labels.values,(224,224),train_tfms)\n",
        "#train_dataset    = CutMix(train_dataset, num_class=8, beta=1.0, prob=0.5, num_mix=2)\n",
        "test_dataset     = Cloader(test_images,X_val.labels.values,(224,224),test_tfms)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset,batch_size=32,shuffle=True,num_workers=0)\n",
        "val_dataloader   = DataLoader(test_dataset,batch_size=32,shuffle=False,num_workers=0)\n",
        "\n",
        "device           = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EkzwBWrPCmN"
      },
      "source": [
        "# ===== Define model\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.base_model=EfficientNet.from_pretrained('efficientnet-b0',num_classes=3)\n",
        "    def forward(self, image, targets):\n",
        "        batch_size, _, _, _ = image.shape\n",
        "        out = self.base_model(image)\n",
        "        targets = torch.tensor(targets,dtype=torch.int64)\n",
        "        loss = nn.CrossEntropyLoss()(out.view(batch_size,3), targets)\n",
        "        return out, loss\n",
        "\n",
        "model = Net()\n",
        "model.to(device);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc_weQvHPCj0"
      },
      "source": [
        "def softmax(array):\n",
        "    return np.exp(array)/np.sum(np.exp(array),axis=1).reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcSuSfGXPChd"
      },
      "source": [
        "optimizer = Ranger(model.parameters(),lr=1e-4)\n",
        "scheduler = ReduceLROnPlateau(optimizer,factor=0.8, mode=\"min\", patience=2)\n",
        "\n",
        "trainer   = Trainer(model=model,optimizer=optimizer,device=device,val_scheduler=scheduler)\n",
        "logger    = Logger()\n",
        "\n",
        "es        = EarlyStop(patience=5,mode=\"min\") # mode = min to minimise loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWmkwlMMPCRE"
      },
      "source": [
        "epochs = 30\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    logger.write(f\"+ ===== Epoch {epoch+1}/{epochs} ===== +\")\n",
        "    train_loss              = trainer.train(train_dataloader)\n",
        "    y_true,y_pred ,val_loss = trainer.evaluate(val_dataloader)\n",
        "    y_pred                  = softmax(y_pred)\n",
        "    accuracy                = accuracy_score(y_true,np.argmax(y_pred,axis=1))\n",
        "    es(val_loss,model,model_path =\"/content/drive/MyDrive/HackNagpur/best.pth\")\n",
        "    logger.write(f\"train_loss {train_loss} val_loss {val_loss} \")\n",
        "    logger.write(f\"val accuracy_score {accuracy} \")\n",
        "    logger.write(\" \")\n",
        "    if es.early_stop:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}